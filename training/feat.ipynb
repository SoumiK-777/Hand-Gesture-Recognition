{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "n_shot = 5\n",
    "n_query = 10\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "n_workers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated JSON file: /home/soumik/Code/cropped/train/classes.json\n",
      "Generated JSON file: /home/soumik/Code/cropped/val/classes.json\n",
      "Generated JSON file: /home/soumik/Code/cropped/test/classes.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "class_names = [\n",
    "    'two_up_inverted',\n",
    "    'dislike',\n",
    "    'two_up',\n",
    "    'mute',\n",
    "    'three2',\n",
    "    'rock',\n",
    "    'four',\n",
    "    'stop',\n",
    "    'ok',\n",
    "    'three',\n",
    "    'stop_inverted',\n",
    "    'peace',\n",
    "    'palm',\n",
    "    'fist',\n",
    "    'like',\n",
    "    'one',\n",
    "    'peace_inverted',\n",
    "    'call'\n",
    "]\n",
    "\n",
    "train_path = '/home/soumik/Code/cropped/train'\n",
    "val_path = '/home/soumik/Code/cropped/val'\n",
    "test_path = '/home/soumik/Code/cropped/test'\n",
    "\n",
    "train_roots = [os.path.join(train_path, class_name) for class_name in class_names]\n",
    "val_roots = [os.path.join(train_path, class_name) for class_name in class_names]\n",
    "test_roots = [os.path.join(train_path, class_name) for class_name in class_names]\n",
    "\n",
    "train_data = {\n",
    "    \"class_names\": class_names,\n",
    "    \"class_roots\": train_roots\n",
    "}\n",
    "val_data = {\n",
    "    \"class_names\": class_names,\n",
    "    \"class_roots\": val_roots\n",
    "}\n",
    "test_data = {\n",
    "    \"class_names\": class_names,\n",
    "    \"class_roots\": test_roots\n",
    "}\n",
    "\n",
    "# Save data to JSON file\n",
    "json_file = '/home/soumik/Code/cropped/train/classes.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "print(f\"Generated JSON file: {json_file}\")\n",
    "\n",
    "json_file = '/home/soumik/Code/cropped/val/classes.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(val_data, f, indent=4)\n",
    "print(f\"Generated JSON file: {json_file}\")\n",
    "\n",
    "json_file = '/home/soumik/Code/cropped/test/classes.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(test_data, f, indent=4)\n",
    "print(f\"Generated JSON file: {json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"call\": 1662,\n",
      "    \"dislike\": 1624,\n",
      "    \"fist\": 1662,\n",
      "    \"four\": 1653,\n",
      "    \"like\": 1603,\n",
      "    \"mute\": 1664,\n",
      "    \"ok\": 1647,\n",
      "    \"one\": 1631,\n",
      "    \"palm\": 1682,\n",
      "    \"peace\": 1643,\n",
      "    \"peace_inverted\": 1730,\n",
      "    \"rock\": 1643,\n",
      "    \"stop\": 1661,\n",
      "    \"stop_inverted\": 1682,\n",
      "    \"three\": 1648,\n",
      "    \"three2\": 1726,\n",
      "    \"two_up\": 1694,\n",
      "    \"two_up_inverted\": 1694\n",
      "}\n",
      "Total samples: 29949\n"
     ]
    }
   ],
   "source": [
    "#Count number of samples in each class of data folder and print the class names and number of samples\n",
    "import os\n",
    "import json\n",
    "\n",
    "data='/home/soumik/Code/cropped/train/'\n",
    "data_val='/home/soumik/Code/cropped/val/'\n",
    "data_test='/home/soumik/Code/cropped/test/'\n",
    "class_names = os.listdir(data)\n",
    "class_names.sort()\n",
    "class_samples = {}\n",
    "for class_name in class_names:\n",
    "    try:\n",
    "        # class_samples[class_name] = len(os.listdir(os.path.join(data, class_name)))\n",
    "        # class_samples[class_name] = len(os.listdir(os.path.join(data_val, class_name)))\n",
    "        class_samples[class_name] = len(os.listdir(os.path.join(data_test, class_name)))\n",
    "    except NotADirectoryError:\n",
    "        pass\n",
    "\n",
    "print(json.dumps(class_samples, indent=4))\n",
    "\n",
    "#Find total samples\n",
    "total_samples = sum(class_samples.values())\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training: 29593\n",
    "Validation: 29449\n",
    "Testing: 29949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfsl.datasets.easy_set import EasySet\n",
    "from torchvision import transforms\n",
    "\n",
    "train_set=EasySet('/home/soumik/Code/cropped/train/classes.json',transform=transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor()]))\n",
    "val_set=EasySet('/home/soumik/Code/cropped/val/classes.json',transform=transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor()]))\n",
    "test_set=EasySet('/home/soumik/Code/cropped/test/classes.json',transform=transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfsl.samplers import TaskSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_tasks_per_epoch = 500\n",
    "n_validation_tasks = 100\n",
    "\n",
    "train_sampler = TaskSampler(\n",
    "    train_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_tasks_per_epoch\n",
    ")\n",
    "val_sampler = TaskSampler(\n",
    "    val_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks\n",
    ")\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks\n",
    ")\n",
    "\n",
    "# Finally, the DataLoader. We customize the collate_fn so that batches are delivered\n",
    "# in the shape: (support_images, support_labels, query_images, query_labels, class_ids)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_sampler.episodic_collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=val_sampler.episodic_collate_fn,\n",
    ")\n",
    "\n",
    "n_test_tasks = 1000\n",
    "\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_test_tasks\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfsl.methods import FEAT\n",
    "from easyfsl.modules import resnet12\n",
    "\n",
    "few_shot_classifier = FEAT.from_resnet12_checkpoint(\"/home/soumik/Code/easy-few-shot-learning/pretrained/feat-1-shot.pth\",device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 21:36:17.038041: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-03 21:36:17.155306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 21:36:17.200603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 21:36:17.213331: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-03 21:36:17.301551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-03 21:36:17.823509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Optimizer\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "scheduler_milestones = [120, 160]\n",
    "scheduler_gamma = 0.1\n",
    "learning_rate = 1e-2\n",
    "tb_logs_dir = Path(\"../logs/proto\")\n",
    "\n",
    "train_optimizer = SGD(\n",
    "    few_shot_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "train_scheduler = MultiStepLR(\n",
    "    train_optimizer,\n",
    "    milestones=scheduler_milestones,\n",
    "    gamma=scheduler_gamma,\n",
    ")\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfsl.methods import FewShotClassifier\n",
    "\n",
    "def training_epoch(\n",
    "    model: FewShotClassifier, data_loader: DataLoader, optimizer: Optimizer\n",
    "):\n",
    "    all_loss = []\n",
    "    model.train()\n",
    "    with tqdm(\n",
    "        enumerate(data_loader), total=len(data_loader), desc=\"Training\"\n",
    "    ) as tqdm_train:\n",
    "        for episode_index, (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            _,\n",
    "        ) in tqdm_train:\n",
    "            optimizer.zero_grad()\n",
    "            model.process_support_set(\n",
    "                support_images.to(DEVICE), support_labels.to(DEVICE)\n",
    "            )\n",
    "            classification_scores = model(query_images.to(DEVICE))\n",
    "\n",
    "            loss = LOSS_FUNCTION(classification_scores, query_labels.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "\n",
    "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:20<00:00, 24.69it/s, loss=0.135]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 58.61it/s, accuracy=0.985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.73it/s, loss=0.0339]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.43it/s, accuracy=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:18<00:00, 26.72it/s, loss=0.024] \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 67.05it/s, accuracy=0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:18<00:00, 26.60it/s, loss=0.0207]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 60.20it/s, accuracy=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.83it/s, loss=0.0129]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 63.60it/s, accuracy=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.14it/s, loss=0.0109] \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.47it/s, accuracy=0.996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.35it/s, loss=0.00888]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 53.31it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.26it/s, loss=0.00686]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 61.55it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.28it/s, loss=0.00878]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 63.05it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.27it/s, loss=0.00697]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 61.19it/s, accuracy=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.13it/s, loss=0.00747]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.05it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 26.28it/s, loss=0.00714]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 56.01it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:18<00:00, 26.39it/s, loss=0.00525]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 60.83it/s, accuracy=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.49it/s, loss=0.00497]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 54.57it/s, accuracy=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.51it/s, loss=0.00403]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.58it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.70it/s, loss=0.00387]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.50it/s, accuracy=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.49it/s, loss=0.00443]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.57it/s, accuracy=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.34it/s, loss=0.00275]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.51it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.88it/s, loss=0.00209]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 65.76it/s, accuracy=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:18<00:00, 26.95it/s, loss=0.00232]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.30it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.85it/s, loss=0.00214]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 56.39it/s, accuracy=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 26.07it/s, loss=0.00333]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 64.00it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.56it/s, loss=0.00397]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 59.77it/s, accuracy=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.28it/s, loss=0.00436]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.83it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.64it/s, loss=0.00532]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 59.01it/s, accuracy=1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.48it/s, loss=0.00437]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.44it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.24it/s, loss=0.00449]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 56.67it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.30it/s, loss=0.00396]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 63.07it/s, accuracy=1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.69it/s, loss=0.00353]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.19it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.69it/s, loss=0.00361]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 58.53it/s, accuracy=0.998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.20it/s, loss=0.00305]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.48it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.90it/s, loss=0.00297]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 65.81it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 26.18it/s, loss=0.004]  \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 65.78it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.50it/s, loss=0.00407]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 58.57it/s, accuracy=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.68it/s, loss=0.00334]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 63.49it/s, accuracy=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.18it/s, loss=0.00818]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.45it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.18it/s, loss=0.00415]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 61.61it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.31it/s, loss=0.00185]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.26it/s, accuracy=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.15it/s, loss=0.00237]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 61.73it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.08it/s, loss=0.0013] \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 53.92it/s, accuracy=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.24it/s, loss=0.00145] \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 55.15it/s, accuracy=1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.47it/s, loss=0.00229]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 61.69it/s, accuracy=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.58it/s, loss=0.00222]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 55.72it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.54it/s, loss=0.0128]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 57.38it/s, accuracy=0.996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.55it/s, loss=0.00993]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 56.55it/s, accuracy=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.41it/s, loss=0.00817]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 61.00it/s, accuracy=0.997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.61it/s, loss=0.00561]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 62.72it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.34it/s, loss=0.00475]\n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 54.79it/s, accuracy=0.995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.96it/s, loss=0.0084] \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 58.27it/s, accuracy=0.996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: 100%|██████████| 500/500 [00:19<00:00, 25.68it/s, loss=0.005]  \n",
      "Validation: 100%|██████████| 100/100 [00:01<00:00, 58.98it/s, accuracy=0.999]\n"
     ]
    }
   ],
   "source": [
    "from easyfsl.utils import evaluate\n",
    "\n",
    "train_losses=[]\n",
    "validation_accuracies=[]\n",
    "best_state = few_shot_classifier.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "model_save_path=\"/home/soumik/Code/easy-few-shot-learning/savedmodels/feat.pth\"\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(few_shot_classifier, train_loader, train_optimizer)\n",
    "    train_losses.append(average_loss)\n",
    "    \n",
    "    validation_accuracy = evaluate(\n",
    "        few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix=\"Validation\"\n",
    "    )\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_state = copy.deepcopy(few_shot_classifier.state_dict())\n",
    "        # state_dict() returns a reference to the still evolving model's state so we deepcopy\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models\n",
    "        torch.save(best_state, model_save_path)\n",
    "        print(\"Ding ding ding! We found a new best model!\")\n",
    "\n",
    "    tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "    tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "    # Warn the scheduler that we did an epoch\n",
    "    # so it knows when to decrease the learning rate\n",
    "    train_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 68.91it/s, accuracy=0.999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy : 99.94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)\n",
    "print(f\"Average accuracy : {(100 * accuracy):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path=\"/home/soumik/Code/easy-few-shot-learning/savedmodels/feat.pth\"\n",
    "few_shot_classifier.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as tt\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from easyfsl.methods import FewShotClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "\n",
    "class FewShotPredictor :\n",
    "    \"\"\"\n",
    "\n",
    "        This class aims to implement a predictor for a Few-shot classifier.\n",
    "\n",
    "        The few shot classifiers need a support set that will be used for calculating the distance between the support set and the query image.\n",
    "\n",
    "        To load the support we have used an ImageFolder Dataset, which needs to have the following structure:\n",
    "\n",
    "        folder:\n",
    "          |_ class_name_folder_1:\n",
    "                 |_ image_1\n",
    "                 |_  …\n",
    "                 |_ image_n\n",
    "          |_ class_name_folder_2:\n",
    "                 |_ image_1\n",
    "                 |_  …\n",
    "                 |_ image_n\n",
    "\n",
    "        The folder must contain the same number of images per class, being the total images (n_way * n_shot).\n",
    "\n",
    "        There must be n_way folders with n_shot images per folder.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self ,\n",
    "                 classifier: FewShotClassifier,\n",
    "                 device,\n",
    "                 path_to_support_images,\n",
    "                 n_way,\n",
    "                 n_shot,\n",
    "                 input_size=28):\n",
    "\n",
    "        \"\"\"\n",
    "            :param classifier: created and loaded model\n",
    "            :param device: device to be executed\n",
    "            :param path_to_support_images: path to creating a support set\n",
    "            :param n_way: number of classes\n",
    "            :param n_shot: number of images on each class\n",
    "            :param input_size: size of image\n",
    "\n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        self.device = device\n",
    "\n",
    "        self.predict_transformation = tt.Compose([\n",
    "            tt.Resize((input_size, input_size)),\n",
    "            tt.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.test_ds = ImageFolder(path_to_support_images, self.predict_transformation)\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size= (n_way*n_shot),\n",
    "            num_workers=1,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        self.support_images, self.support_labels = next(iter(self.val_loader))\n",
    "\n",
    "\n",
    "\n",
    "    def predict (self, tensor_normalized_image):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tensor_normalized_image:\n",
    "        Example of normalized image:\n",
    "\n",
    "            pil_img = PIL.Image.open(img_dir)\n",
    "\n",
    "            torch_img = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])(pil_img)\n",
    "\n",
    "            tensor_normalized_image = tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(torch_img)[None]\n",
    "\n",
    "\n",
    "        :return:\n",
    "\n",
    "        Return\n",
    "\n",
    "        predict = tensor with prediction (mean distance of query image and support set)\n",
    "        torch_max [1] = predicted class index\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "           self.classifier.eval()\n",
    "           self.classifier.to(self.device)\n",
    "           self.classifier.process_support_set(self.support_images.to(self.device), self.support_labels.to(self.device))\n",
    "           pre_predict = self.classifier(tensor_normalized_image.to(self.device))\n",
    "           predict = pre_predict.detach().data\n",
    "           torch_max = torch.max(predict,1)\n",
    "           class_name = self.test_ds.classes[torch_max[1].item()]\n",
    "           return predict, torch_max[1], class_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-20.0112,  -9.7762, -22.2944]], device='cuda:0'),\n",
       " tensor([1], device='cuda:0'),\n",
       " '2')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize the FewShotPredictor\n",
    "predictor = FewShotPredictor(few_shot_classifier, DEVICE, \"/home/soumik/Code/easy-few-shot-learning/support\", 3, 5, 28)\n",
    "\n",
    "pil_img = PIL.Image.open(\"/home/soumik/Code/easy-few-shot-learning/support/2/00a4406e-d2bd-4110-8855-4e0d659b729b.jpg\")\n",
    "\n",
    "torch_img = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor()\n",
    "    ])(pil_img)\n",
    "\n",
    "tensor_normalized_image = tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(torch_img)[None]\n",
    "FewShotPredictor(few_shot_classifier,DEVICE,\"/home/soumik/Code/easy-few-shot-learning/support\",3,5,28).predict(tensor_normalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@10.072] global cap_v4l.cpp:1136 tryIoctl VIDEOIO(V4L2:/dev/video0): select() timeout.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def predict_frame(frame, predictor):\n",
    "    # Convert the frame to a PIL image\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Preprocess the image\n",
    "    torch_img = preprocess(pil_img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move the image tensor to the device\n",
    "    tensor_normalized_image = torch_img.to(DEVICE)\n",
    "    \n",
    "    # Predict using the FewShotPredictor\n",
    "    prediction = predictor.predict(tensor_normalized_image)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the FewShotPredictor\n",
    "predictor = FewShotPredictor(few_shot_classifier, DEVICE, \"/home/soumik/Code/easy-few-shot-learning/support\", 3, 5, 28)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Predict the frame\n",
    "    prediction = predict_frame(frame, predictor)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.putText(frame, f'Prediction: {prediction}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x79473877e0b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss and accuracy of a model using loss array and val_loss array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I got a list of losses and valacccuracies from the training process\n",
    "loss = train_losses\n",
    "val_loss = validation_accuracies\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation accuracy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
